# E-Commerce Data Warehouse Pipeline

An end-to-end **data engineering pipeline** for e-commerce analytics, built with **Docker**, **Apache Spark**, **Hive**, and **Jenkins** for automated orchestration.

**Status:** Red Phase Completed (Infrastructure & Data Ingestion Layer)

---

## Table of Contents
- [Tech Stack](#tech-stack)
- [Architecture](#architecture)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Data Flow](#data-flow)
- [Key Concepts](#key-concepts)
- [Development](#development)

---

## ğŸ›  Tech Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| **Containerization** | Docker & Docker Compose | Latest |
| **Distributed Storage** | Apache Hadoop HDFS | 3.2.1 |
| **Data Warehouse** | Apache Hive | 2.3.2 |
| **Processing Engine** | Apache Spark | 3.5.0 |
| **Job Orchestration** | Jenkins | Latest |
| **Language** | Python (PySpark) | 3.x |
| **Metadata Store** | PostgreSQL | 9.6+ |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Jenkins Orchestrator                     â”‚
â”‚            (Scheduled Jobs & Pipeline Automation)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼              â–¼              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Generatorsâ”‚  â”‚Generatorsâ”‚  â”‚ Spark    â”‚
   â”‚ (Products,â”‚  â”‚(Logs, TX)â”‚  â”‚ Jobs     â”‚
   â”‚Countries) â”‚  â”‚(PyScript)â”‚  â”‚(PySpark) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚         â”‚             â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   HDFS (Data Lake)       â”‚
          â”‚  /user/root/ecommerce/   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Hive Metastore (PostgreSQL) â”‚
        â”‚   Metadata & Governance      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Hive Warehouse Database   â”‚
        â”‚   ecommerce_db Tables      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Start

### Prerequisites
- **Docker Desktop** installed and running
- Minimum **6GB RAM** allocated to Docker
- **~5-10 minutes** initialization time on first start

### Step 1: Start the Cluster

```bash
cd docker/
docker-compose up -d --build
```

**Expected output:**
- 10+ containers starting (Hadoop, Hive, Spark, Jenkins, etc.)
- Check status: `docker-compose ps`

### Step 2: Initialize Infrastructure

1. Open **Jenkins** at [http://localhost:9090](http://localhost:9090)
   - Initial Admin Password: Check Jenkins logs or use default
   - `docker logs jenkins | grep -i "initial admin"`

2. Run **Setup_Infrastructure** job
   - This initializes HDFS directories, Hive database schema, and creates tables
   - Logs available in Jenkins UI

### Step 3: Access Web UIs

| Service | URL | Purpose |
|---------|-----|---------|
| **Jenkins** | [http://localhost:9090](http://localhost:9090) | Job orchestration & scheduling |
| **Spark Master** | [http://localhost:8080](http://localhost:8080) | Spark cluster monitoring |
| **HDFS NameNode** | [http://localhost:9870](http://localhost:9870) | Storage administration |
| **ResourceManager** | [http://localhost:8088](http://localhost:8088) | Hadoop YARN resource tracking |
| **Hue** | [http://localhost:8888](http://localhost:8888) | Hive query editor & database browser |

---

## Project Structure

```
data-warehouse-e-commerce/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yaml          # Full cluster definition
â”‚   â”œâ”€â”€ Dockerfile.jenkins            # Custom Jenkins with docker-in-docker
â”‚   â”œâ”€â”€ hadoop.env                    # Hadoop configuration
â”‚   â”œâ”€â”€ hadoop-hive.env               # Hive configuration
â”‚   â”œâ”€â”€ jenkins_home/                 # Jenkins persistent state
â”‚   â”‚   â””â”€â”€ jobs/
â”‚   â”‚       â”œâ”€â”€ Setup_Infrastructure/ # Initialize system
â”‚   â”‚       â”œâ”€â”€ Daily_Data_Pipeline/  # Master data refresh
â”‚   â”‚       â””â”€â”€ Hourly_Data_Pipeline/ # Transaction ingestion
â”‚   â””â”€â”€ conf.dist/                    # Hue & logging configs
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ processing/                   # Spark ETL Jobs
â”‚   â”‚   â”œâ”€â”€ ingest_logs.py            # Ingest raw transaction logs
â”‚   â”‚   â”œâ”€â”€ ingest_products.py        # Ingest product master data (SCD Type 2)
â”‚   â”‚   â”œâ”€â”€ ingest_countries.py       # Ingest country master data (SCD Type 0)
â”‚   â”‚   â””â”€â”€ metastore_db/             # Internal Spark Derby metastore
â”‚   â”‚
â”‚   â””â”€â”€ script/
â”‚       â”œâ”€â”€ init_hive.sql             # Hive schema creation
â”‚       â”œâ”€â”€ data_generator.py          # Transaction log generator
â”‚       â”œâ”€â”€ product_and_countries_generator.py  # Master data generator
â”‚       â”œâ”€â”€ generator/                 # Generator utilities
â”‚       â””â”€â”€ ingestion/                 # Ingestion utilities
â”‚
â”œâ”€â”€ data/                              # Local data staging (CSV before HDFS)
â”œâ”€â”€ DIARY.md                           # Engineering decisions & troubleshooting log
â”œâ”€â”€ README.md                          # This file
â””â”€â”€ LICENSE
```

---

## Data Flow & Pipelines

### Data Pipeline Schedule

| Job | Frequency | Triggers | Output |
|-----|-----------|----------|--------|
| **Setup_Infrastructure** | On-demand | Manual trigger via Jenkins | Initialize database & HDFS |
| **Hourly_Data_Pipeline** | Every hour | Cron schedule | Generates and ingests raw logs |
| **Daily_Data_Pipeline** | Every morning (6 AM) | Cron schedule | Refreshes products & countries |

### Data Ingestion Flow

#### 1. **Transaction Logs (Hourly)**
```
Generator â†’ CSV Files (HDFS) â†’ Spark Ingestion Job â†’ transactions_raw (Parquet)
```
- **Schema:** `invoice_no, stock_code, quantity, invoice_date, customer_id, country_id, region_id, ingestion_timestamp`
- **Validation:** Validates Foreign Keys (Products & Countries exist in warehouse)
- **Mode:** APPEND (append-only fact table)

#### 2. **Product Master Data (Daily)**
```
Generator â†’ CSV Files (HDFS) â†’ Spark Ingestion Job â†’ products (Parquet)
```
- **Schema:** `stock_code, description, unit_price, price_date`
- **Type:** Slowly Changing Dimension Type 2 (keeps history)
- **Mode:** OVERWRITE with lineage break technique (collect â†’ repartition â†’ overwrite)

#### 3. **Country Master Data (Daily)**
```
Generator â†’ CSV Files (HDFS) â†’ Spark Ingestion Job â†’ countries (Parquet)
```
- **Schema:** `country_id, country_name, continent`
- **Type:** Slowly Changing Dimension Type 0 (no history)
- **Mode:** OVERWRITE

---

## ğŸ”‘ Key Concepts & Patterns

### Data Storage
- **HDFS Path Structure:** `/user/root/ecommerce/{logs,products,countries}/`
- **Hive Database:** `ecommerce_db`
- **Format:** Parquet (columnar, compressed)
- **Warehouse Dir:** `/user/hive/warehouse`

### Data Integrity Features
1. **Idempotent Jobs:** All ingestion jobs can be safely re-run without duplicating data
2. **Foreign Key Validation:** Transaction generator fetches real Product IDs from Hive before creating logs
3. **Timestamp Tracking:** Each record includes `ingestion_timestamp` for data lineage
4. **Self-Healing:** Spark configured with extended timeouts and retry logic for zombie process handling

### SCD Implementation
- **Type 0 (countries):** Overwrite completely (latest values only)
- **Type 2 (products):** Keep full history (lineage break + collect-to-driver technique)

### Critical Configuration
- **Spark Metastore:** Connected to Hive via `thrift://hive-metastore:9083`
- **Spark Worker:** Limited to 1 core, 512MB memory (development setup)
- **Timeouts:** `spark.network.timeout=120s`, `spark.rpc.askTimeout=60s` (for zombie handling)

---

## Development

### Running Jobs Locally

**Spark SQL in Container:**
```bash
docker exec -it spark-master spark-sql \
  --master spark://spark-master:7077 \
  --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083
```

**Viewing HDFS:**
```bash
docker exec -it namenode hdfs dfs -ls /user/root/ecommerce/
```

**Querying Hive in Container:**
```bash
docker exec -it hive-server hive -e "SELECT * FROM ecommerce_db.transactions_raw LIMIT 10;"
```

### Common Tasks

**Check Pipeline Status:**
- Jenkins: [http://localhost:9090](http://localhost:9090) â†’ View job build history

**Monitor Spark Jobs:**
- Spark Master: [http://localhost:8080](http://localhost:8080) â†’ Applications

**Browse HDFS:**
- NameNode: [http://localhost:9870](http://localhost:9870) â†’ Utilities â†’ Browse HDFS

**Query Data (Easy UI):**
- Hue: [http://localhost:8888](http://localhost:8888) â†’ New Query

### Troubleshooting

See **DIARY.md** for detailed engineering decisions and known issues:
- Hive metastore persistence on Docker restart
- Spark hanging with "Initial job has not accepted resources"
- Container timezone & DNS resolution

---

## Project Phases

| Phase | Status | Scope |
|-------|--------|-------|
| **Red** | Completed | Infrastructure setup & data ingestion |
| **Green** | Next | Create data marts (aggregations) |
| **Yellow** | Planned | Build dimensional data warehouse |
| **Purple** | Planned | Implement alerting & SLA monitoring |
| **Blue** | Planned | BI dashboard & finalization |

---

## Additional Resources

- **Engineering Log:** See `DIARY.md` for detailed problem-solving notes and design decisions
- **Spark Documentation:** [Apache Spark 3.5.0](https://spark.apache.org/docs/3.5.0/)
- **Hive Documentation:** [Apache Hive 2.3.2](https://cwiki.apache.org/confluence/display/Hive)
- **Docker Reference:** [Docker Compose](https://docs.docker.com/compose/)

---

**Repository:** `data-warehouse-e-commerce` | **Branch:** `feature/red-phase`
