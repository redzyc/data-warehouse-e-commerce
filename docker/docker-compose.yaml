services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    networks:
      - hadoopnet
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864"
    env_file:
      - ./hadoop.env
    ports:
      - "8088:8088"
    networks:
      - hadoopnet

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    ports:
      - "8188:8188"
    networks:
      - hadoopnet

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoopnet

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore-postgresql/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
    networks:
      - hadoopnet

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"
    networks:
      - hadoopnet

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    networks:
      - hadoopnet
    volumes:
      - hive_metastore_db:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  spark-master:
    image: apache/spark:3.5.0-python3
    container_name: spark-master
    restart: always
    user: root 
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - HADOOP_CONF_DIR=/opt/hadoop/conf
      - SPARK_CONF_spark_network_timeout=120s
      - SPARK_CONF_spark_rpc_askTimeout=60s
      - SPARK_CONF_spark_worker_timeout=120
      - SPARK_CONF_spark_deploy_defaultCores=1 
      - SPARK_CONF_spark_executor_cores=1    
      - SPARK_CONF_spark_executor_memory=512m
    ports:
      - "8080:8080" 
      - "7077:7077" 
    volumes:
      - ../src/processing:/opt/spark/jobs
      - ../src/script:/opt/spark/scripts
      - ../data:/opt/spark/data
      - ./spark-jars:/opt/spark/jars-custom
      - ./.env:/opt/spark/.env
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    networks:
      hadoopnet:
        aliases:
          - spark
          - spark-master

  spark-worker1:
    image: apache/spark:3.5.0-python3
    container_name: spark-worker-1
    user: root
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_CONF_spark_network_timeout=120s
      - SPARK_CONF_spark_rpc_askTimeout=60s
      - SPARK_CONF_spark_worker_timeout=120
      - SPARK_CONF_spark_executor_cores=1
      - SPARK_CONF_spark_executor_memory=512m
    volumes:
      - ../src/processing:/opt/spark/jobs
      -  ./spark-jars:/opt/spark/jars-custom
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    networks:
      - hadoopnet

  hue:
    image: gethue/hue:20201001-135001
    container_name: hue
    hostname: hue
    ports:
      - "8888:8888"
    volumes:
      - ./conf.dist:/usr/share/hue/desktop/conf
    depends_on:
      - namenode
    networks:
      - hadoopnet

  jenkins:
    build: 
      context: .
      dockerfile: Dockerfile.jenkins
    container_name: jenkins
    user: root
    ports:
      - "9090:8080"
      - "50000:50000"
    volumes:
      - ./jenkins_home:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock
      - ../src/script:/opt/scripts
    networks:
      - hadoopnet
  superset:
    image: apache/superset:4.0.2
    container_name: superset
    restart: always
    depends_on:
      - hive-metastore-postgresql
    environment:
      - SUPERSET_DATABASE_HOST=hive-metastore-postgresql
      - SUPERSET_SECRET_KEY=tajni_kljuc112233
      - GUNICORN_KEEP_ALIVE=120
      - SUPERSET_SQLALCHEMY_DATABASE_URI=postgresql+psycopg2://hive:hive@hive-metastore-postgresql:5432/superset
      - SUPERSET_DATABASE_DB=superset
      - SUPERSET_CONFIG_PATH=/app/pythonpath/superset_config.py
      
    ports:
      - "8089:8088"
    volumes:
      - ./superset/superset_config.py:/app/pythonpath/superset_config.py
    command: >
      sh -c "
      superset fab create-admin --username admin --firstname Admin --lastname Adminovic --email admin@admin.com --password admin &&
      superset db upgrade &&
      superset init &&
      /usr/bin/run-server.sh"
    networks:
      - hadoopnet


networks:
  hadoopnet:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hive_metastore_db: